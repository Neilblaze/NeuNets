# Implementation of Neural Nets and Optimizers üìù

## Structure 
Implementations of a few neural network layers: 
- [Linear](./src/layers/linear.py) ‚Üí Linear (fully connected) layer 
- [ReLU](./src/layers/relu.py) ‚Üí Layer ReLU activation function 
- [Sigmoid](./src/layers/sigmoid.py) ‚Üí Sigmoid activation function layer 
- [Dropout](./src/layers/dropout.py) ‚Üí Dropout layer (random dropout of neurons) 
- [LogSoftmax](./src/layers/log_softmax.py) ‚Üí LogSoftmax layer (for multiclass classification) 
- [RNN](./src/layers/rnn.py) ‚Üí Recurrent layer (Vanilla RNN) 
- [GRU](./src/layers/gru.py) ‚Üí Recurrent GRU layer 

Available loss functions:
- [NLLLoss](./src/criterions/neg_log_likelihood_loss.py) ‚Üí Negative Log Likelihood Loss 
- [CrossEntropyLoss](./src/criterions/cross_entropy_loss.py) ‚Üí CrossEntropyLoss represented as LogSoftmax + NLLLoss 
- [FocalLoss](./src/criterions/focal_loss.py) ‚Üí FocalLoss: $-(1-p)^\gamma * log(p)$ 

Optimizers: 
- [SGD](./src/optimizers/sgd.py) ‚Üí Stochastic Gradient Descent method 
- [AdaGrad](./src/optimizers/adagrad.py) ‚Üí AdaGrad method 
- [RMSprop](./src/optimizers/rmsprop.py) ‚Üí RMSprop method
